{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Important Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"Transformations.jpg\">\n",
    "<img src = \"Transformations1.jpg\">\n",
    "<img src = \"Transformations2.jpg\">\n",
    "<img src = \"Transformations3.jpg\">\n",
    "<img src = \"Transformations4.jpg\">\n",
    "<img src = \"Transformations5.jpg\">\n",
    "<img src = \"Transformations6.jpg\">\n",
    "<img src = \"Transformations7.jpg\">\n",
    "<img src = \"Transformations8.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction to Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"Introduction to CNN.jpg\">\n",
    "<img src = \"Introduction to CNN1.jpg\">\n",
    "<img src = \"Introduction to CNN2.jpg\">\n",
    "<img src = \"Introduction to CNN3.jpg\">\n",
    "<img src = \"Introduction to CNN4.jpg\">\n",
    "<img src = \"Introduction to CNN5.jpg\">\n",
    "<img src = \"Introduction to CNN6.jpg\">\n",
    "<img src = \"Introduction to CNN7.jpg\">\n",
    "<img src = \"Introduction to CNN8.jpg\">\n",
    "<img src = \"Introduction to CNN9.jpg\">\n",
    "<img src = \"Introduction to CNN10.jpg\">\n",
    "<img src = \"Introduction to CNN11.jpg\">\n",
    "<img src = \"Introduction to CNN12.jpg\">\n",
    "<img src = \"Introduction to CNN13.jpg\">\n",
    "<img src = \"Introduction to CNN14.jpg\">\n",
    "<img src = \"Introduction to CNN15.jpg\">\n",
    "<img src = \"Introduction to CNN16.jpg\">\n",
    "<img src = \"Introduction to CNN17.jpg\">\n",
    "<img src = \"Introduction to CNN18.jpg\">\n",
    "<img src = \"Introduction to CNN19.jpg\">\n",
    "<img src = \"Introduction to CNN20.jpg\">\n",
    "<img src = \"Introduction to CNN21.jpg\">\n",
    "<img src = \"Introduction to CNN22.jpg\">\n",
    "<img src = \"Introduction to CNN23.jpg\">\n",
    "<img src = \"Introduction to CNN24.jpg\">\n",
    "<img src = \"Introduction to CNN25.jpg\">\n",
    "<img src = \"Introduction to CNN26.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "# Machine Learning Foundation\n",
    "\n",
    "## Course 5, Part e: CNN DEMO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a CNN to classify images in the CIFAR-10 Dataset\n",
    "\n",
    "We will work with the CIFAR-10 Dataset.  This is a well-known dataset for image classification, which consists of 60000 32x32 color images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
    "\n",
    "The 10 classes are:\n",
    "\n",
    "<ol start=\"0\">\n",
    "<li> airplane\n",
    "<li>  automobile\n",
    "<li> bird\n",
    "<li>  cat\n",
    "<li> deer\n",
    "<li> dog\n",
    "<li>  frog\n",
    "<li>  horse\n",
    "<li>  ship\n",
    "<li>  truck\n",
    "</ol>\n",
    "\n",
    "For details about CIFAR-10 see:\n",
    "https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "\n",
    "For a compilation of published performance results on CIFAR 10, see:\n",
    "http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html\n",
    "\n",
    "---\n",
    "\n",
    "### Building Convolutional Neural Nets\n",
    "\n",
    "In this exercise we will build and train our first convolutional neural networks.  In the first part, we walk through the different layers and how they are configured.  In the second part, you will build your own model, train it, and compare the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import optimizers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 836s 5us/step\n",
      "170508288/170498071 [==============================] - 836s 5us/step\n",
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# The data, shuffled and split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Each image is a 32 x 32 x 3 numpy array\n",
    "x_train[444].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAc3klEQVR4nO2da2yc53Xn/2duHN5EkSIlKxIt2YmBJshufGENFwmKtEULJ1vUCVAEyYfAQIMoWNTABmg/GF5gk120RVo0CfKhSKHURt0izaW5IN6Ft21qdNdN0TqWfJHteNexHfkiU3fxzhnO5fTDjFHZff6H1JAcyn7+P0DQ8Dl83vfM875nhvP855xj7g4hxNufwk47IIToDwp2ITJBwS5EJijYhcgEBbsQmaBgFyITSpuZbGa3A/gKgCKAP3P3L0S/Pzy6y8enptLGt7AEaLDAxp+Xt9vBPM7w0CC1FUvp1+92O/AjWProqkSyLbOFcwIf29E6Bk62mR/hMwtWP7xNA2N0QYnRenDxwrmzWFpYSFp7DnYzKwL4EwC/CuBVAI+a2QPu/hM2Z3xqCnf9jz9I2rzV5OciixgFGZzbCgVuC298TwdnuVimc4reorbWyjK1lYMbZ+aW91Lb7t27kuPLq2t0TqPFX3QCE5ot/twajUZyfG0tPQ4A9Vqd2mpNfq61wI96M31f1dv8fit4kdoQrEf4ghT8DV2w9P1Y5k8LhUL6gL9/z+/yOfxw63IrgOfd/UV3XwPwTQB3bOJ4QohtZDPBfgDAK5f9/Gp3TAhxFbLtG3RmdsTMjpnZseWFhe0+nRCCsJlgPwVg+rKfD3bH3oC7H3X3GXefGd6V/jwphNh+NhPsjwK4wcyuM7MKgI8DeGBr3BJCbDU978a7e9PM7gLwt+hIb/e5+zPRHIPRneumBTugZLcy0jMKwbZ6tENeDvSOAtltbdT5rnqjVqO2UrC1e2h6mtomh/llK7XTvuwaG6JzPFx7rjR0XuPTFArpYzJFAwCaZOccANaC3fOVJt/hP3X2YnL85dNn6BxYEBbtSGblPhYL/HkXLG0bGuJrv2diIjk+UA7uDWrZAO7+IIAHN3MMIUR/0DfohMgEBbsQmaBgFyITFOxCZIKCXYhM2NRufC9Q4SLMvErPKgSvVQVwea0QyDjttRVqq9fSslaFZJoBwMG9e6jtumsPUds1k5PUVlu+QG2LJLlmoBEkGgWJPEYkNAAoFPjtUwzmMaJMtFJwPUcDuWmkkr42hSZPDEKRX89Sia9VtcT9GBvmMuXE+Eh6fGyUH29sLDk+WA3kUGoRQrytULALkQkKdiEyQcEuRCYo2IXIhL7uxhuAIklqaQcJEix5InLeGzwBxRur1FYKkhmm9qRTdA9fy5NW9u3bR21DVZ6c0g7KMC0F5ZvqDbKO1UC5iBI/gh3ygvMdbWuReTSpCWFNsGI7KO9V58dsrKRrKEyNpXfAAaBY4delWq1S2/guXhtwYhc/5sjwQHI8EHlQKhGFKip/xU1CiLcTCnYhMkHBLkQmKNiFyAQFuxCZoGAXIhP6nAjjAGl5VAo7dKRt7RpPWhkM8jD27EknEQDA/iBxZR+xDQXtmHptDcXaFgFAPeiq0mASVZCYUixHiTCB9Gb8mjEZLe5oFFibfB3bgSzXbKRlyum9e+mc4RFeBblY4us4MMBtZSKVAUE3pKA24JVXZdQ7uxDZoGAXIhMU7EJkgoJdiExQsAuRCQp2ITJhU9KbmZ0EsAigBaDp7jPR77tzmaFdW6LzSiS76h2kdhcATF/Ds80mp3h9t+ogz04qFK48Yy+ST8IMMIvq6/Hzsay9KEOtGNwGRQTyT/C0mQhkwXOOZLm1qKRdm69VkaSBDZb5Aceq0ckCL4MFKQV1/th9UK6ks+EAoEzq3Vlw32yFzv5L7n5+C44jhNhG9Ge8EJmw2WB3AH9nZsfN7MhWOCSE2B42+2f8B9z9lJntBfBDM/t/7v7w5b/QfRE4AgDje/hnZSHE9rKpd3Z3P9X9/yyA7wO4NfE7R919xt1nhkf5d46FENtLz8FuZsNmNvr6YwC/BuDprXJMCLG1bObP+H0Avt+VUkoA/srd/yaaUCw4dlXS0kVUfHH/3mvTDozzvxRGRoa5H0X+tFmrKQBwIr0hkKciCa0dSGjtoN2RGZd/jBwzSLrCQPiaz59bKzhmoUWeWzuQruj6Agiy75xkRXampdexEshkhaj4aeRiICuyQqsAUCim17gQZCpGbbkYPQe7u78I4H29zhdC9BdJb0JkgoJdiExQsAuRCQp2ITJBwS5EJvS14GSlVMS1U6NJ28F9vNDjwFA6u43JKgDQiqSJoCFWlJVVIPM8KA4ZZbbF8wL5J3iNdpJlVyJZUsA6mW2FIFsrakZWSxfFLAVzmj1k8wGhuokyOR/rH9g5Xm/ZiFGxRwvu1QI5pgcZdpGNnueKZwgh3pIo2IXIBAW7EJmgYBciExTsQmRCX3fjC2aoVtN1tdg4ANQb6fpp5WDXlO1wAnFrpSiZ4cr3P2NYTbv1bBapCSTR5MK5s3TOYInX8kOpws8V1Go798pr6cMFKsnCCq9DuLLCW30NB0lPLdJubHCQP+fqaLRzzu+CYnDPeYOrCex+rAY16HpB7+xCZIKCXYhMULALkQkKdiEyQcEuRCYo2IXIhL5KbwCXGVpBYkKRJXEEc5jkAsQSWjuYV6S1wnp7zYySbiJbscjP11pL+//Uk0/QOYevfRe11Zp8tRZry9T27BNPJccvXLhA5yytcnltaZ7bFpa4ZHfN9MHk+PT119E5t/38LdQ2EkjExSDJ5/rrD1EbEzfrdd6yq1RKX+dQVqYWIcTbCgW7EJmgYBciExTsQmSCgl2ITFCwC5EJ60pvZnYfgF8HcNbd39sdmwDwLQCHAZwE8DF3v7TesRxB7awgy4uKYVENt6h+VzAvskVyGCOS5UI/Av+jzDw00rXfli/xy9N+R43aBiqD1FYdGKO2VSJ5DQ9V6Rwn0iYA1JZ4Jtr//ccfUdvwaNrHobHddM7CMpcUDx14B7U99vhxajtwYB+1DQ6lW581m0HdPXYPbFJ6+3MAt79p7G4AD7n7DQAe6v4shLiKWTfYu/3WL75p+A4A93cf3w/gI1vrlhBiq+n1M/s+d5/tPj6NTkdXIcRVzKY36LzzwZN+UDCzI2Z2zMyOzc/Nb/Z0Qoge6TXYz5jZfgDo/k9rHrn7UXefcfeZsd18Q0cIsb30GuwPALiz+/hOAD/YGneEENvFRqS3bwD4IIBJM3sVwOcAfAHAt83sUwBeAvCxjZ6wTRSDKFunTYr8RRKUBc14es02YzJar8cLZb7A/2jeHMkq8zUur60scllupfnmvdl/o76alvkA4NK588nxR3/8CJ2zFnVdci7ZLa1yqeylV15Ojt/ygdvonIsX+XOen+cfRatV7mMlKB5JC2YWeeutYjEdupHUu26wu/sniOlX1psrhLh60DfohMgEBbsQmaBgFyITFOxCZIKCXYhM6HvByeCrdnwSsUVTCsHrWK9SGbP1ItetR8+Zee10dli1xDPKlgPp7ewcl7VW5uvUNjU5mRwfGQ76sgUFG1u0LCNwoHqA2tokm/KFnz5H51yzZ4Lann/+eWobGUlnrwFAMboPyOV00rcPALxw5Z0H9c4uRCYo2IXIBAW7EJmgYBciExTsQmSCgl2ITOiv9GYAjGSORXIS6+kWymTcjVJQ2LCXopLtFi+G2Gzwfl21Gpeu6vXAVgsKRFbTBSIPHryWzrm4MEdt7SZ/biOjI9T2H26+KTn+7ptupHMGguM5+DVbXeNrtdZKF22sN3nGXtWCsGjxXoADw7w4Z4NPw8pK+noODPIsOtZ3MELv7EJkgoJdiExQsAuRCQp2ITJBwS5EJvR1N97d0PL0bncx7OSU3soM8gTQCGqutdt8a7RB2icBfIe8FuycR+eK2vtE7atKQcLI0Nh4ek6B1zNrgNuGxvZS2xRp8QQA11x/ODk+ufcaOqdcCnwMWjJZhe9Mnzp3Ojl+/ny6Vh8AoMbXPhBe0Ax23F96Je0HAAyV0/7vGefqxN796TZUHtxvemcXIhMU7EJkgoJdiExQsAuRCQp2ITJBwS5EJmyk/dN9AH4dwFl3f2937PMAPg3gXPfX7nH3B9c7VrvdxvLKatJ2ejY9DgCNRlqiWmsGEkmQgBLVhYtsLEkmmjM0xOuSjY6OUtvAAG8XdOEC7aOJSjHty/AAT9JoBVkaE3vTteQAYO+7DlPb0nL6etbWgutCkqQA4IXnf0ptB6+bprZXfnYyOX7sX/6Fzlld4LJt0XnIWJCc4kWeYFUdTF/r6YNc9rzxlpnk+Fq0vtTyb/w5gNsT41929xu7/9YNdCHEzrJusLv7wwB4pzshxFuCzXxmv8vMTpjZfWaW/tqWEOKqoddg/yqAdwK4EcAsgC+yXzSzI2Z2zMyOLQTtboUQ20tPwe7uZ9y95e5tAF8DcGvwu0fdfcbdZ3aNjfXqpxBik/QU7Ga2/7IfPwrg6a1xRwixXWxEevsGgA8CmDSzVwF8DsAHzexGdFKzTgL4zEZO5t6mmWOXVlfovHIpLU2UKrxG11CVy1qRHDY4yCUqJoeVSnwZe7VFtfDm53jGVpu0fxrbvZvOWZxboLYGq/8HYGCIr1WFXJtKibdxKkQ1BYmkCAAe1IVbmUt/dDzz4st0zuoKz2KM6tOVgyTG+TV+f7dG0/dVscBT7A4eOp8cjzIp1w12d/9EYvje9eYJIa4u9A06ITJBwS5EJijYhcgEBbsQmaBgFyIT+lpw0goFDA6mZa/p8Qk6j8k4xTKX3sqBVBNJXh60oWJEMll0vKgYpQcFJ0MTOd+u3fwLTWvX8Oyq8/OXqK1FshEBYGxoV3K8vsoLejYCCa1FJEUAeO655/i8evp85Ta/Zq0Ct41VeTZitc4vTD2Q3urkVh0d4QUnX3vtVHK8EWV7UosQ4m2Fgl2ITFCwC5EJCnYhMkHBLkQmKNiFyIT+Sm9mVPaqBtlmTmSSqLhelK0VSWWtoJlXnZyvGfSHi+S16FyRzVv8fKMjaWmzVuNFFCNZrjLMr0t7hR/z0qV0bzYjGYwAUA7ONTvLe6WtrvI+cCBZYK0gO6y+youfzq3xtS/V+TGXG/yY9aX0MRcWF+mcQjkdR9F9o3d2ITJBwS5EJijYhcgEBbsQmaBgFyIT+rob32o2cfFiun7ak7Mv0nlsQ7u+FhT9CnbBe23/1CC77lGyS7TzHxH5MTnBd88HKulLurjEd3b3TPIWT3zvHPjb7/yA2k48+nhyfHL6WjrnE5/5LWqzIDmlGrTKqpPkmgb4/VEql/nxqAVYLgTtyEiLJwAAuUdWA7WjOpy2tdvcB72zC5EJCnYhMkHBLkQmKNiFyAQFuxCZoGAXIhM20v5pGsBfANiHTvWzo+7+FTObAPAtAIfRaQH1MXfnBcsANFstzM+nWw2dnj1J55UH0rXmmi0uMwwEdeaiFk+RVNYmElskrkXH6zUhp9ngtqWldFLIAll3AGgFMuXyJd559/jD/0RtJx57IjneHkpLcgAw80vvp7bJiT3UthTIimbF5PiBQ4foHAT3FSq8fVUjfSoAwBppewYARbL8N7zrBjqnZel7oFTkTmzknb0J4Hfc/T0AbgPw22b2HgB3A3jI3W8A8FD3ZyHEVcq6we7us+7+WPfxIoBnARwAcAeA+7u/dj+Aj2yTj0KILeCKPrOb2WEANwF4BMA+d5/tmk6j82e+EOIqZcPBbmYjAL4L4LPu/oYPgN75vmjyg46ZHTGzY2Z2bGlxaVPOCiF6Z0PBbmZldAL96+7+ve7wGTPb37XvB3A2Ndfdj7r7jLvPjIzyovdCiO1l3WC3zpbxvQCedfcvXWZ6AMCd3cd3AuBZEUKIHWcjWW/vB/BJAE+Z2RPdsXsAfAHAt83sUwBeAvCx9Q7UbjuWVtK1uJ4+8Qydt0CyzZpR+6GoxVPQ+qcRqC51Ioe1g3pmHrV4Cs7VDtodVUpc/rFmuk5euc1rpx0+xDPRKkW+jpcWLlLbNQfHk+PNQKf8n9/4OrWNjfEWVecWuKxYI9emtswzyqLahst1XkvOAym1ZPx9dWUhLR2efHk2OQ4AH/5PH0qOW4FLb+sGu7v/CFxK/pX15gshrg70DTohMkHBLkQmKNiFyAQFuxCZoGAXIhP6WnDSW23Ul9LSxVOPn6DzXj2fTqYrFPlr1aE9E9S2vMQzkM4TGQQA2uW0rFGINLSAXjPivM2f9wgxTQ1zuW7h9Hlq2zW2i9rGx9PZiAAwPjmVHK+SDEYAOHcu+b0sAMBzz5yktpfOnaO2RdauyYO1D94CPbAdDoppRhLmiz97OTn+2mm+Hk8+9ZPk+OzsGTpH7+xCZIKCXYhMULALkQkKdiEyQcEuRCYo2IXIhL5KbzBDqZDuo3Vw30E6rbaczhxbWOYyWVQ0cM8u3iutHGSUnV2YS4570JetVyLprRjYdo+OJsf3jvNaAqWgZOZAmd8ik1O8CORqPV2oxIOsrOg5z5G1B4DVGs9ga5CsQwve51pNnql46DpeqPI37riD2n72Au9leI5Ih02S7QkAZ86cTs9p8jl6ZxciExTsQmSCgl2ITFCwC5EJCnYhMqG/iTAA2F7hyO7ddN7u3eld9+WVFTqnUeN14YbTggAAYO84T6C5OJ9OyInq1iHYYY7wILnG29xWr6WTfObm+HpUS3xBBqr8FmkHde3ed8vNyfHVZZ6EdO7McWprBHX+WFsuAGh5eme9EGW7FPg1qzd4fbqXXk4ntADALNk9B4A6qXkX1TZE4cqTr/TOLkQmKNiFyAQFuxCZoGAXIhMU7EJkgoJdiExYV3ozs2kAf4FOS2YHcNTdv2JmnwfwaQCvf4v/Hnd/MDxWwVAYTJ9ycCKdwAEAq8+lEx0sqEHnQXLHKmlBtR4DpXQSRzuQ15qkZRSwTp25SHqjFqBJ2kYZSUACgOrgID+X8aSQSP6ZPnxdcrzF1To8+s9cemsFbbSKpDYgABSIehUlwjj4NTsb1Lt78G/+N7U1g5ZSzXp6Ucy5H+OT6WSui/Ncjt6Izt4E8Dvu/piZjQI4bmY/7Nq+7O5/vIFjCCF2mI30epsFMNt9vGhmzwI4sN2OCSG2liv6zG5mhwHcBOCR7tBdZnbCzO4zs3TbTiHEVcGGg93MRgB8F8Bn3X0BwFcBvBPAjei883+RzDtiZsfM7NjyUrqggRBi+9lQsJtZGZ1A/7q7fw8A3P2Mu7fcvQ3gawBuTc1196PuPuPuM8MjvFqKEGJ7WTfYrbNlfC+AZ939S5eN77/s1z4K4Omtd08IsVVsZDf+/QA+CeApM3uiO3YPgE+Y2Y3oKEEnAXxmvQMVzDBaTdd4O3yY16B7+vjjxMKln2YgXdVZSyAAhSKXw/ZOTSbHa0Uu/bx66jVqi+F+BN2f0CK2yhBvuzQ2yWvJVUo888oC6e1l8rwPTV9P55SC7LtIiqxU+XNrNtPyVa3GpbAoU7EVSKlLK8v8kIFeyhTkqBbeIImjQlAPcSO78T9C+s4LNXUhxNWFvkEnRCYo2IXIBAW7EJmgYBciExTsQmRCXwtOrq2u4GdPPpm0lVs8W2diKJ2VdSEqDBgVKAwyqHyVzxsoD6fnBMULo8w2BHJSNK0d2OqttP9zy/zbi8Uyl7x2DXNZcQ94tlyTFMWcm1vgc4JrFmU4RhlxRu6RgYEB7keb+9EI0vbMgwsTXU9yH3jwVlxfTWduerAWemcXIhMU7EJkgoJdiExQsAuRCQp2ITJBwS5EJvRVeltaWMSPHvo/SdtgmWsTRjSIygDPdlpY4hlIleAlLuiuhcWLrFAll65GAlkrkgDbLW6LMvpYptTFeb4e8wtc9hys8utSCZrm3TSSLoh4+hWeBbiywAuBkuQ1AECtzvvHOclIHBwc4n7UgxS14Jr12tevTVLi2kX+pJ2cKypGqnd2ITJBwS5EJijYhcgEBbsQmaBgFyITFOxCZEJfpbdGs4mzZ0mvrEBOGhpKyySVMnd/fJRnZI2OcFuV9KIDOgUzUxTbfE7UU6xFMtQ6Ni67tAv8fPVG+pjNBs/WimS+Wp1Ldq+8donalufTWXYL5y/SOQuLXHpbDoqENgO9yYhUtrrK5UbSLg8AUAwy28KstyDtzS19QucJh1gh/QojOVfv7EJkgoJdiExQsAuRCQp2ITJBwS5EJqy7G29mVQAPAxjo/v533P1zZnYdgG8C2APgOIBPunvQUweolEo4uG8qaRsJmj5WB9MJL8MVvl1ZBnelVA5qxgUtjVgLomaDJ4REu+qBABGVLEPL+PMmpd/CWniNYKf+zJkz1FZf4rvnxx99NG0IWhot1vjO/0qLX892Kdi29vT5Wk3+nEtBrkspeH+MWi9F7auYbbjIw3OQ2JhiBGzsnb0O4Jfd/X3otGe+3cxuA/CHAL7s7u8CcAnApzZwLCHEDrFusHuH10XTcvefA/hlAN/pjt8P4CPb4aAQYmvYaH/2YreD61kAPwTwAoA5d3/9GxyvAjiwLR4KIbaEDQW7u7fc/UYABwHcCuDnNnoCMztiZsfM7Fgj+PwqhNhermg33t3nAPwDgF8AsNvMXt8lOAjgFJlz1N1n3H2mHPQxF0JsL+sGu5lNmdnu7uNBAL8K4Fl0gv43u792J4AfbJOPQogtYCOJMPsB3G9mRXReHL7t7v/LzH4C4Jtm9nsAHgdw73oHqg5U8O53Tidt5UqFziuSvwjKQcW4YlAXrh1kOvSSnBLVrWsFLaoiWS6SytoIatdRhYdLP5UKP9eBqQlqa6xxOay2nJbRVoN6cfMrvEVVKXhbKgStoaqkzZMFMhm/E4HB4K/TqKVUqRQlWKXHq0Gi18hwOjnstYtcvlw32N39BICbEuMvovP5XQjxFkDfoBMiExTsQmSCgl2ITFCwC5EJCnYhMsGibJwtP5nZOQAvdX+cBHC+byfnyI83Ij/eyFvNj0Punkwt7Wuwv+HEZsfcfWZHTi4/5EeGfujPeCEyQcEuRCbsZLAf3cFzX478eCPy4428bfzYsc/sQoj+oj/jhciEHQl2M7vdzP6/mT1vZnfvhA9dP06a2VNm9oSZHevjee8zs7Nm9vRlYxNm9kMz+2n3//Ed8uPzZnaquyZPmNmH++DHtJn9g5n9xMyeMbP/0h3v65oEfvR1TcysamY/NrMnu3789+74dWb2SDduvmVmUYLev8fd+/oPQBGdslbXo5NN+CSA9/Tbj64vJwFM7sB5fxHAzQCevmzsjwDc3X18N4A/3CE/Pg/gd/u8HvsB3Nx9PArgOQDv6feaBH70dU3QKS480n1cBvAIgNsAfBvAx7vjfwrgP1/JcXfinf1WAM+7+4veKT39TQB37IAfO4a7PwzgzR0O70CncCfQpwKexI++4+6z7v5Y9/EiOsVRDqDPaxL40Ve8w5YXed2JYD8A4JXLft7JYpUO4O/M7LiZHdkhH15nn7vPdh+fBrBvB325y8xOdP/M3/aPE5djZofRqZ/wCHZwTd7kB9DnNdmOIq+5b9B9wN1vBvAhAL9tZr+40w4BnVd2RKVltpevAngnOj0CZgF8sV8nNrMRAN8F8Fl3X7jc1s81SfjR9zXxTRR5ZexEsJ8CcHltKlqscrtx91Pd/88C+D52tvLOGTPbDwDd/8/uhBPufqZ7o7UBfA19WhMzK6MTYF939+91h/u+Jik/dmpNuueewxUWeWXsRLA/CuCG7s5iBcDHATzQbyfMbNjMRl9/DODXADwdz9pWHkCncCewgwU8Xw+uLh9FH9bEOgX37gXwrLt/6TJTX9eE+dHvNdm2Iq/92mF8027jh9HZ6XwBwH/dIR+uR0cJeBLAM/30A8A30PlzsIHOZ69PodMz7yEAPwXw9wAmdsiPvwTwFIAT6ATb/j748QF0/kQ/AeCJ7r8P93tNAj/6uiYA/iM6RVxPoPPC8t8uu2d/DOB5AH8NYOBKjqtv0AmRCblv0AmRDQp2ITJBwS5EJijYhcgEBbsQmaBgFyITFOxCZIKCXYhM+FexY9ZdqzYxEQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Let's look at one of the images\n",
    "\n",
    "print(y_train[444])\n",
    "plt.imshow(x_train[444]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "\n",
    "y_train = keras.utils.np_utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.np_utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now instead of classes described by an integer between 0-9 we have a vector with a 1 in the (Pythonic) 9th position\n",
    "y_train[444]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As before, let's make everything float and scale\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Layers for CNNs\n",
    "- Previously we built Neural Networks using primarily the Dense, Activation and Dropout Layers.\n",
    "\n",
    "- Here we will describe how to use some of the CNN-specific layers provided by Keras\n",
    "\n",
    "### Conv2D\n",
    "\n",
    "```python\n",
    "keras.layers.convolutional.Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n",
    "```\n",
    "\n",
    "A few parameters explained:\n",
    "- `filters`: the number of filter used per location.  In other words, the depth of the output.\n",
    "- `kernel_size`: an (x,y) tuple giving the height and width of the kernel to be used\n",
    "- `strides`: and (x,y) tuple giving the stride in each dimension.  Default is `(1,1)`\n",
    "- `input_shape`: required only for the first layer\n",
    "\n",
    "Note, the size of the output will be determined by the kernel_size, strides\n",
    "\n",
    "### MaxPooling2D\n",
    "`keras.layers.pooling.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)`\n",
    "\n",
    "- `pool_size`: the (x,y) size of the grid to be pooled.\n",
    "- `strides`: Assumed to be the `pool_size` unless otherwise specified\n",
    "\n",
    "### Flatten\n",
    "Turns its input into a one-dimensional vector (per instance).  Usually used when transitioning between convolutional layers and fully connected layers.\n",
    "\n",
    "---\n",
    "\n",
    "## First CNN\n",
    "Below we will build our first CNN.  For demonstration purposes (so that it will train quickly) it is not very deep and has relatively few parameters.  We use strides of 2 in the first two convolutional layers which quickly reduces the dimensions of the output.  After a MaxPooling layer, we flatten, and then have a single fully connected layer before our final classification layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 16, 16, 32)        2432      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 6, 6, 32)          25632     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 3, 3, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 3, 3, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 288)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               147968    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 181,162\n",
      "Trainable params: 181,162\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Let's build a CNN using Keras' Sequential capabilities\n",
    "\n",
    "model_1 = Sequential()\n",
    "\n",
    "\n",
    "## 5x5 convolution with 2x2 stride and 32 filters\n",
    "model_1.add(Conv2D(32, (5, 5), strides = (2,2), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model_1.add(Activation('relu'))\n",
    "\n",
    "## Another 5x5 convolution with 2x2 stride and 32 filters\n",
    "model_1.add(Conv2D(32, (5, 5), strides = (2,2)))\n",
    "model_1.add(Activation('relu'))\n",
    "\n",
    "## 2x2 max pooling reduces to 3 x 3 x 32\n",
    "model_1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_1.add(Dropout(0.25))\n",
    "\n",
    "## Flatten turns 3x3x32 into 288x1\n",
    "model_1.add(Flatten())\n",
    "model_1.add(Dense(512))\n",
    "model_1.add(Activation('relu'))\n",
    "model_1.add(Dropout(0.5))\n",
    "model_1.add(Dense(num_classes))\n",
    "model_1.add(Activation('softmax'))\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have 181K parameters, even though this is a \"small\" model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vijay\\anaconda3\\envs\\data_science\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1563/1563 [==============================] - 34s 21ms/step - loss: 1.4857 - accuracy: 0.4648 - val_loss: 1.3072 - val_accuracy: 0.5253\n",
      "Epoch 2/15\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 1.3534 - accuracy: 0.5193 - val_loss: 1.2261 - val_accuracy: 0.5675\n",
      "Epoch 3/15\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 1.2737 - accuracy: 0.5473 - val_loss: 1.2408 - val_accuracy: 0.5556\n",
      "Epoch 4/15\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 1.2206 - accuracy: 0.5699 - val_loss: 1.1369 - val_accuracy: 0.5987\n",
      "Epoch 5/15\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 1.1798 - accuracy: 0.5816 - val_loss: 1.0690 - val_accuracy: 0.6209\n",
      "Epoch 6/15\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 1.1494 - accuracy: 0.5946 - val_loss: 1.1468 - val_accuracy: 0.5974\n",
      "Epoch 7/15\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 1.1241 - accuracy: 0.6091 - val_loss: 1.0884 - val_accuracy: 0.6178\n",
      "Epoch 8/15\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.1103 - accuracy: 0.6127 - val_loss: 1.0553 - val_accuracy: 0.6285\n",
      "Epoch 9/15\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 1.0887 - accuracy: 0.6206 - val_loss: 1.0211 - val_accuracy: 0.6449\n",
      "Epoch 10/15\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 1.0766 - accuracy: 0.6249 - val_loss: 0.9974 - val_accuracy: 0.6543\n",
      "Epoch 11/15\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.0657 - accuracy: 0.6298 - val_loss: 1.0239 - val_accuracy: 0.6428\n",
      "Epoch 12/15\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.0570 - accuracy: 0.6365 - val_loss: 1.0235 - val_accuracy: 0.6384\n",
      "Epoch 13/15\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 1.0532 - accuracy: 0.6377 - val_loss: 1.0385 - val_accuracy: 0.6389\n",
      "Epoch 14/15\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.0482 - accuracy: 0.6397 - val_loss: 1.0532 - val_accuracy: 0.6442\n",
      "Epoch 15/15\n",
      "1563/1563 [==============================] - 27s 18ms/step - loss: 1.0473 - accuracy: 0.6404 - val_loss: 1.0584 - val_accuracy: 0.6426\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bb0d5d4a58>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = tensorflow.keras.optimizers.RMSprop(lr=0.0005, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model_1.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_1.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=15,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 8, 8, ..., 5, 1, 7], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "predict_x = model_1.predict(x_test)\n",
    "np.argmax(predict_x,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6426"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(np.argmax(y_test,axis=1),np.argmax(predict_x,axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Our previous model had the structure:\n",
    "\n",
    "Conv -> Conv -> MaxPool -> (Flatten) -> Dense -> Final Classification\n",
    "\n",
    "(with appropriate activation functions and dropouts)\n",
    "\n",
    "1. Build a more complicated model with the following pattern:\n",
    "- Conv -> Conv -> MaxPool -> Conv -> Conv -> MaxPool -> (Flatten) -> Dense -> Final Classification\n",
    "\n",
    "- Use strides of 1 for all convolutional layers.\n",
    "\n",
    "2. How many parameters does your model have?  How does that compare to the previous model?\n",
    "\n",
    "3. Train it for 5 epochs.  What do you notice about the training time, loss and accuracy numbers (on both the training and validation sets)?\n",
    "\n",
    "5. Try different structures and run times, and see how accurate your model can be.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build a CNN using Keras' Sequential capabilities\n",
    "\n",
    "model_2 = Sequential()\n",
    "\n",
    "model_2.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model_2.add(Activation('relu'))\n",
    "model_2.add(Conv2D(32, (3, 3)))\n",
    "model_2.add(Activation('relu'))\n",
    "model_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_2.add(Dropout(0.25))\n",
    "\n",
    "model_2.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model_2.add(Activation('relu'))\n",
    "model_2.add(Conv2D(64, (3, 3)))\n",
    "model_2.add(Activation('relu'))\n",
    "model_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model_2.add(Dropout(0.25))\n",
    "\n",
    "model_2.add(Flatten())\n",
    "model_2.add(Dense(512))\n",
    "model_2.add(Activation('relu'))\n",
    "model_2.add(Dropout(0.5))\n",
    "model_2.add(Dense(num_classes))\n",
    "model_2.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 30, 30, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 15, 15, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 13, 13, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               1180160   \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,250,858\n",
      "Trainable params: 1,250,858\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## Check number of parameters\n",
    "\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate RMSprop optimizer\n",
    "opt_2 = tensorflow.keras.optimizers.RMSprop(lr=0.0005)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model_2.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt_2,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1563/1563 [==============================] - 232s 148ms/step - loss: 1.2550 - accuracy: 0.5544 - val_loss: 1.0554 - val_accuracy: 0.6315\n",
      "Epoch 2/5\n",
      "1563/1563 [==============================] - 218s 139ms/step - loss: 1.0451 - accuracy: 0.6337 - val_loss: 0.9752 - val_accuracy: 0.6585\n",
      "Epoch 3/5\n",
      "1563/1563 [==============================] - 196s 125ms/step - loss: 0.9314 - accuracy: 0.6760 - val_loss: 0.8342 - val_accuracy: 0.7082\n",
      "Epoch 4/5\n",
      "1563/1563 [==============================] - 212s 135ms/step - loss: 0.8711 - accuracy: 0.7014 - val_loss: 0.9856 - val_accuracy: 0.6706\n",
      "Epoch 5/5\n",
      "1563/1563 [==============================] - 172s 110ms/step - loss: 0.8350 - accuracy: 0.7158 - val_loss: 0.8341 - val_accuracy: 0.7209\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1babfc51ef0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=5,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"transfer Learning.jpg\">\n",
    "<img src = \"transfer Learning1.jpg\">\n",
    "<img src = \"transfer Learning2.jpg\">\n",
    "<img src = \"transfer Learning3.jpg\">\n",
    "<img src = \"transfer Learning4.jpg\">\n",
    "<img src = \"transfer Learning5.jpg\">\n",
    "<img src = \"transfer Learning6.jpg\">\n",
    "<img src = \"transfer Learning7.jpg\">\n",
    "<img src = \"transfer Learning8.jpg\">\n",
    "<img src = \"transfer Learning9.jpg\">\n",
    "<img src = \"transfer Learning10.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "# Machine Learning Foundation\n",
    "\n",
    "## Course 5, Part g: Transfer Learning DEMO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, we will use the well-known MNIST digit data. To illustrate the power and concept of transfer learning, we will train a CNN on just the digits 5,6,7,8,9.  Then we will train just the last layer(s) of the network on the digits 0,1,2,3,4 and see how well the features learned on 5-9 help with classifying 0-4.\n",
    "\n",
    "Adapted from https://github.com/fchollet/keras/blob/master/examples/mnist_transfer_cnn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "#import keras\n",
    "#from keras.datasets import mnist\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "#from keras.layers import Conv2D, MaxPooling2D\n",
    "#from keras import backend as K\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used to help some of the timing functions\n",
    "now = datetime.datetime.now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some parameters\n",
    "batch_size = 128\n",
    "num_classes = 5\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some more parameters\n",
    "img_rows, img_cols = 28, 28\n",
    "filters = 32\n",
    "pool_size = 2\n",
    "kernel_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This just handles some variability in how the input data is loaded\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To simplify things, write a function to include all the training steps\n",
    "## As input, function takes a model, training set, test set, and the number of classes\n",
    "## Inside the model object will be the state about which layers we are freezing and which we are training\n",
    "\n",
    "def train_model(model, train, test, num_classes):\n",
    "    x_train = train[0].reshape((train[0].shape[0],) + input_shape)\n",
    "    x_test = test[0].reshape((test[0].shape[0],) + input_shape)\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "    print('x_train shape:', x_train.shape)\n",
    "    print(x_train.shape[0], 'train samples')\n",
    "    print(x_test.shape[0], 'test samples')\n",
    "\n",
    "    # convert class vectors to binary class matrices\n",
    "    y_train = keras.utils.to_categorical(train[1], num_classes)\n",
    "    y_test = keras.utils.to_categorical(test[1], num_classes)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adadelta',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    t = now()\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              verbose=1,\n",
    "              validation_data=(x_test, y_test))\n",
    "    print('Training time: %s' % (now() - t))\n",
    "\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test score:', score[0])\n",
    "    print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# create two datasets: one with digits below 5 and one with 5 and above\n",
    "x_train_lt5 = x_train[y_train < 5]\n",
    "y_train_lt5 = y_train[y_train < 5]\n",
    "x_test_lt5 = x_test[y_test < 5]\n",
    "y_test_lt5 = y_test[y_test < 5]\n",
    "\n",
    "x_train_gte5 = x_train[y_train >= 5]\n",
    "y_train_gte5 = y_train[y_train >= 5] - 5\n",
    "x_test_gte5 = x_test[y_test >= 5]\n",
    "y_test_gte5 = y_test[y_test >= 5] - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the \"feature\" layers.  These are the early layers that we expect will \"transfer\"\n",
    "# to a new problem.  We will freeze these layers during the fine-tuning process\n",
    "\n",
    "feature_layers = [\n",
    "    Conv2D(filters, kernel_size,\n",
    "           padding='valid',\n",
    "           input_shape=input_shape),\n",
    "    Activation('relu'),\n",
    "    Conv2D(filters, kernel_size),\n",
    "    Activation('relu'),\n",
    "    MaxPooling2D(pool_size=pool_size),\n",
    "    Dropout(0.25),\n",
    "    Flatten(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the \"classification\" layers.  These are the later layers that predict the specific classes from the features\n",
    "# learned by the feature layers.  This is the part of the model that needs to be re-trained for a new problem\n",
    "\n",
    "classification_layers = [\n",
    "    Dense(128),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes),\n",
    "    Activation('softmax')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create our model by combining the two sets of layers as follows\n",
    "model = Sequential(feature_layers + classification_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_10 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 26, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               589952    \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 5)                 645       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 600,165\n",
      "Trainable params: 600,165\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (29404, 28, 28, 1)\n",
      "29404 train samples\n",
      "4861 test samples\n",
      "Epoch 1/5\n",
      "230/230 [==============================] - 43s 183ms/step - loss: 1.6023 - accuracy: 0.2193 - val_loss: 1.5848 - val_accuracy: 0.2526\n",
      "Epoch 2/5\n",
      "230/230 [==============================] - 37s 161ms/step - loss: 1.5790 - accuracy: 0.2714 - val_loss: 1.5588 - val_accuracy: 0.3423\n",
      "Epoch 3/5\n",
      "230/230 [==============================] - 37s 160ms/step - loss: 1.5539 - accuracy: 0.3233 - val_loss: 1.5299 - val_accuracy: 0.4495\n",
      "Epoch 4/5\n",
      "230/230 [==============================] - 37s 160ms/step - loss: 1.5268 - accuracy: 0.3761 - val_loss: 1.4971 - val_accuracy: 0.5561\n",
      "Epoch 5/5\n",
      "230/230 [==============================] - 40s 176ms/step - loss: 1.4960 - accuracy: 0.4352 - val_loss: 1.4593 - val_accuracy: 0.6484\n",
      "Training time: 0:03:14.294884\n",
      "Test score: 1.4592978954315186\n",
      "Test accuracy: 0.6484262347221375\n"
     ]
    }
   ],
   "source": [
    "# Now, let's train our model on the digits 5,6,7,8,9\n",
    "\n",
    "train_model(model,\n",
    "            (x_train_gte5, y_train_gte5),\n",
    "            (x_test_gte5, y_test_gte5), num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freezing Layers\n",
    "Keras allows layers to be \"frozen\" during the training process.  That is, some layers would have their weights updated during the training process, while others would not.  This is a core part of transfer learning, the ability to train just the last one or several layers.\n",
    "\n",
    "Note also, that a lot of the training time is spent \"back-propagating\" the gradients back to the first layer.  Therefore, if we only need to compute the gradients back a small number of layers, the training time is much quicker per iteration.  This is in addition to the savings gained by being able to train on a smaller data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze only the feature layers\n",
    "for l in feature_layers:\n",
    "    l.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe below the differences between the number of *total params*, *trainable params*, and *non-trainable params*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_10 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 26, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               589952    \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 5)                 645       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 600,165\n",
      "Trainable params: 590,597\n",
      "Non-trainable params: 9,568\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (30596, 28, 28, 1)\n",
      "30596 train samples\n",
      "5139 test samples\n",
      "Epoch 1/5\n",
      "240/240 [==============================] - 16s 65ms/step - loss: 1.5697 - accuracy: 0.2980 - val_loss: 1.5352 - val_accuracy: 0.4207\n",
      "Epoch 2/5\n",
      "240/240 [==============================] - 15s 64ms/step - loss: 1.5219 - accuracy: 0.3696 - val_loss: 1.4847 - val_accuracy: 0.5011\n",
      "Epoch 3/5\n",
      "240/240 [==============================] - 15s 64ms/step - loss: 1.4760 - accuracy: 0.4427 - val_loss: 1.4349 - val_accuracy: 0.5995\n",
      "Epoch 4/5\n",
      "240/240 [==============================] - 16s 65ms/step - loss: 1.4311 - accuracy: 0.5051 - val_loss: 1.3858 - val_accuracy: 0.6762\n",
      "Epoch 5/5\n",
      "240/240 [==============================] - 16s 66ms/step - loss: 1.3873 - accuracy: 0.5683 - val_loss: 1.3375 - val_accuracy: 0.7416\n",
      "Training time: 0:01:19.057217\n",
      "Test score: 1.3375095129013062\n",
      "Test accuracy: 0.7415839433670044\n"
     ]
    }
   ],
   "source": [
    "train_model(model,\n",
    "            (x_train_lt5, y_train_lt5),\n",
    "            (x_test_lt5, y_test_lt5), num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that after a single epoch, we are already achieving results on classifying 0-4 that are comparable to those achieved on 5-9 after 5 full epochs.  This despite the fact the we are only \"fine-tuning\" the last layer of the network, and all the early layers have never seen what the digits 0-4 look like.\n",
    "\n",
    "Also, note that even though nearly all (590K/600K) of the *parameters* were trainable, the training time per epoch was still much reduced.  This is because the unfrozen part of the network was very shallow, making backpropagation faster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "- Now we will write code to reverse this training process.  That is, train on the digits 0-4, then finetune only the last layers on the digits 5-9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_12 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 26, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               589952    \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 5)                 645       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 600,165\n",
      "Trainable params: 600,165\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create layers and define the model as above\n",
    "feature_layers2 = [\n",
    "    Conv2D(filters, kernel_size,\n",
    "           padding='valid',\n",
    "           input_shape=input_shape),\n",
    "    Activation('relu'),\n",
    "    Conv2D(filters, kernel_size),\n",
    "    Activation('relu'),\n",
    "    MaxPooling2D(pool_size=pool_size),\n",
    "    Dropout(0.25),\n",
    "    Flatten(),\n",
    "]\n",
    "\n",
    "classification_layers2 = [\n",
    "    Dense(128),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes),\n",
    "    Activation('softmax')\n",
    "]\n",
    "model2 = Sequential(feature_layers2 + classification_layers2)\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (30596, 28, 28, 1)\n",
      "30596 train samples\n",
      "5139 test samples\n",
      "Epoch 1/5\n",
      "240/240 [==============================] - 46s 189ms/step - loss: 1.5908 - accuracy: 0.2508 - val_loss: 1.5621 - val_accuracy: 0.3565\n",
      "Epoch 2/5\n",
      "240/240 [==============================] - 47s 197ms/step - loss: 1.5543 - accuracy: 0.3140 - val_loss: 1.5205 - val_accuracy: 0.5001\n",
      "Epoch 3/5\n",
      "240/240 [==============================] - 46s 191ms/step - loss: 1.5146 - accuracy: 0.3958 - val_loss: 1.4741 - val_accuracy: 0.5916\n",
      "Epoch 4/5\n",
      "240/240 [==============================] - 42s 174ms/step - loss: 1.4684 - accuracy: 0.4695 - val_loss: 1.4198 - val_accuracy: 0.6558\n",
      "Epoch 5/5\n",
      "240/240 [==============================] - 39s 164ms/step - loss: 1.4156 - accuracy: 0.5373 - val_loss: 1.3564 - val_accuracy: 0.7293\n",
      "Training time: 0:03:40.181389\n",
      "Test score: 1.3563551902770996\n",
      "Test accuracy: 0.7293247580528259\n"
     ]
    }
   ],
   "source": [
    "# Now, let's train our model on the digits 0,1,2,3,4\n",
    "train_model(model2,\n",
    "            (x_train_lt5, y_train_lt5),\n",
    "            (x_test_lt5, y_test_lt5), num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Freeze layers\n",
    "for l in feature_layers2:\n",
    "    l.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_12 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 26, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               589952    \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 5)                 645       \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 600,165\n",
      "Trainable params: 590,597\n",
      "Non-trainable params: 9,568\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (29404, 28, 28, 1)\n",
      "29404 train samples\n",
      "4861 test samples\n",
      "Epoch 1/5\n",
      "230/230 [==============================] - 14s 59ms/step - loss: 1.5979 - accuracy: 0.2574 - val_loss: 1.5652 - val_accuracy: 0.3440\n",
      "Epoch 2/5\n",
      "230/230 [==============================] - 15s 65ms/step - loss: 1.5634 - accuracy: 0.3041 - val_loss: 1.5273 - val_accuracy: 0.4036\n",
      "Epoch 3/5\n",
      "230/230 [==============================] - 13s 58ms/step - loss: 1.5299 - accuracy: 0.3572 - val_loss: 1.4906 - val_accuracy: 0.4871\n",
      "Epoch 4/5\n",
      "230/230 [==============================] - 13s 56ms/step - loss: 1.4951 - accuracy: 0.4112 - val_loss: 1.4553 - val_accuracy: 0.5993\n",
      "Epoch 5/5\n",
      "230/230 [==============================] - 13s 56ms/step - loss: 1.4626 - accuracy: 0.4746 - val_loss: 1.4208 - val_accuracy: 0.6700\n",
      "Training time: 0:01:08.354253\n",
      "Test score: 1.4207820892333984\n",
      "Test accuracy: 0.6700267195701599\n"
     ]
    }
   ],
   "source": [
    "train_model(model2,\n",
    "            (x_train_gte5, y_train_gte5),\n",
    "            (x_test_gte5, y_test_gte5), num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convolutional Neural Network Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"Architectures.jpg\">\n",
    "<img src = \"Architectures1.jpg\">\n",
    "<img src = \"Architectures2.jpg\">\n",
    "<img src = \"Architectures3.jpg\">\n",
    "<img src = \"Architectures4.jpg\">\n",
    "<img src = \"Architectures5.jpg\">\n",
    "<img src = \"Architectures6.jpg\">\n",
    "<img src = \"Architectures7.jpg\">\n",
    "<img src = \"Architectures8.jpg\">\n",
    "<img src = \"Architectures9.jpg\">\n",
    "<img src = \"Architectures10.jpg\">\n",
    "<img src = \"Architectures11.jpg\">\n",
    "<img src = \"Architectures12.jpg\">\n",
    "<img src = \"Architectures13.jpg\">\n",
    "<img src = \"Architectures14.jpg\">\n",
    "<img src = \"Architectures15.jpg\">\n",
    "<img src = \"Architectures16.jpg\">\n",
    "<img src = \"Architectures17.jpg\">\n",
    "<img src = \"Architectures18.jpg\">\n",
    "<img src = \"Architectures19.jpg\">\n",
    "<img src = \"Architectures20.jpg\">\n",
    "<img src = \"Architectures21.jpg\">\n",
    "<img src = \"Architectures22.jpg\">\n",
    "<img src = \"Architectures23.jpg\">\n",
    "<img src = \"Architectures24.jpg\">\n",
    "<img src = \"Architectures25.jpg\">\n",
    "<img src = \"Architectures26.jpg\">\n",
    "<img src = \"Architectures27.jpg\">\n",
    "<img src = \"Architectures28.jpg\">\n",
    "<img src = \"Architectures29.jpg\">\n",
    "<img src = \"Architectures30.jpg\">\n",
    "<img src = \"Architectures31.jpg\">\n",
    "<img src = \"Architectures32.jpg\">\n",
    "<img src = \"Architectures33.jpg\">\n",
    "<img src = \"Architectures34.jpg\">\n",
    "<img src = \"Architectures35.jpg\">\n",
    "<img src = \"Architectures36.jpg\">\n",
    "<img src = \"Architectures37.jpg\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.12 ('data_science')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "66aefaa15347530fd336e5ff4c6baa78eb01ac4bdb41d42482c99492d43d4e3d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
